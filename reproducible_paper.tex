\section{A reproducible paper}

\subsection{Rationale}

In devising a procedure that might be adopted as a minimum communication standard, it was important to first consider why computational scientists do not currently publish their code. The literature on this topic is relatively sparse, however a recent survey of the machine learning community found that the top reason for not sharing code was the perceived time required to prepare it for publication, followed by the prospect of dealing with questions from users  \citep{Stodden2010}. While many researchers are sympathetic to the ideals of open science and reproducible research, it appears that the practicalities seem too difficult and time consuming, particularly when the pressure to publish is so strong and unrelenting.

The bewildering array of suggested tools and best practices is one reason why an individual working in the weather and climate sciences might place reproducibility in the `too hard' basket. An appropriate solution for any given scientist no doubt exists within that collection of tools and practices, but it is heavily obscured. Consider the regular scientist described in Box 1. In consulting the literature on reproducible computational research, they would be confronted with options including data provenance tracking systems like VisTrails \citep{Freire2012} and PyRDM \citep{Jacobs2014}, software environment managers like Docker and Vagrant \citep{Stodden2014}, and even online services where your code and data can be run by others \citep{Stodden2012}. These might be fantastic options for small teams of software engineers or experienced scientific programmers who may be dealing with very large workflows (e.g. post-processing of thousands of CMIP5 model runs), complex model simulations and/or production style code (e.g. they might be developing a satellite retrieval algorithm that has high re-use potential in the wider community), but a regular scientist has neither the requisite computational experience or a research problem of sufficient scale and complexity to necessarily require and/or make use of such tools.

The procedure employed by IS2015 was designed with this regular scientist in mind. It looked to minimize both the time involved and the complexity of the associated tools, while at the same time remaining faithful to established best practices in scientific computing.
  
\subsection{Components}

At first glance, the only difference between a regular journal article and that of IS2015 is the addition of short computation section (Box 2). This section accompanied the traditional description of data and methods within the article and briefly cited the major software packages used in the research, before pointing the reader to three key supplementary items: (1) a more detailed description of the software used, (2) a version controlled and publicly available code repository and (3) a collection of supplementary log files that captured the data processing steps taken in producing each key result. The repository was hosted at a popular code sharing website called GitHub, while the detailed software description and log files were hosted at Figshare \citep{Irving2015}, which is a website where researchers commonly archive the `long tail' of their research (e.g. supplementary figures, code and data). 

\subsubsection{Software description}

There is an important difference between citing the software that was used in a study (i.e. so that the authors get appropriate academic credit) and describing it in sufficient detail so as to convey the precise version and computing environment \citep{Jackson2012}. Recognising this, IS2015 began their computation section with a high-level description of the software used, which included citations to any papers written about the software. Authors of scientific software are increasingly publishing with journals like the \textit{Journal of Open Research Software}, so it is important for users of that software to cite those papers within their manuscripts. This high-level description is also useful for briefly articulating what general tasks each software item was used for (e.g. plotting, data analysis, file manipulation). Such an overview does not provide sufficient detail to recreate the computing environment used in the study, so IS2015 provided a link to a supplementary file on Figshare that documents the precise version of each software package used and the operating system upon which it was run (namely the name, version number, release date, institution and DOI or URL). 

\subsubsection{Code repository}

An important best practice in scientific computing is that of modularizing code, rather than copying and pasting \citep{Wilson2014a}. As such, any communication standard for code should expect/encourage authors to provide a whole library of code (i.e. a repository containing many interconnected scripts) as opposed to a single script for each key figure or result. A related best practice is the use of a version control system like Git, Subversion or Mercurial. These systems are easily linked to an online hosting service such as GitHub or Bitbucket, which is the means by which a code repository can be made publicly available. Most of the examples of reproducible research currently available in the literature produced a pristine GitHub or Bitbucket repository for their paper (i.e. one that contains only the code that is directly relevant to that paper), however this should not be an expectation for regular scientists who are not looking for broad-scale community uptake of their code. Not only is this a time consuming practice that would likely involve a degree of cutting and pasting, it also goes against the workflow that version control promotes. By providing a link to their everyday repository instead, authors can quickly and easily signal to readers what code was used to produce the results in their paper. The authors may further develop that code in the future, so the reader then also has easy access to the latest version if they would like to use it in their own work and/or suggest improvements (referred to as a `pull request' in version control parlance). There are all sorts of extra bits and pieces of code in the everyday repository that is linked to by IS2015. Readers will probably never look at that code (because it is not referred to in the associated log files), but what is the harm if they do? Students and other scientists could potentially learn from it, and in the best case scenario they would inform the authors of a bug or potential improvement to the code.        

In addition to providing a link to the associated Github repository, IS2015 provided (on Figshare) a static snapshot of the repository taken at the time the manuscript was accepted for publication. The motivation for doing this was not so much to obtain the Digital Object Identifier (DOI) that comes with Figshare pages (they are not looking for community uptake and lots of citations), but rather to have a static version of the code available in case the associated GitHub repository is ever moved or deleted. Since this snapshot does not provide a revision history of the code, it would not have been appropriate to provide it \textit{instead} of the link to GitHub. As the log file in Box 3 demonstrates, not all of the results shown in a paper will necessarily have been generated with the very latest version of the code, hence the need for the revision history. In other words, the version controlled repository is of critical importance, while the static snapshot is simply an additional backup measure.  

\subsubsection{Log files}\label{s:log_files}

A code repository and software description on its own is not much use to a reader; they also need to know how that code was used in generating the results presented in the paper. It turns out that in the weather and climate sciences, the answer to adequately documenting the computational steps involved in producing a given result has been staring us in the face for years. As a community we have almost universally adopted a self-describing file format (i.e. a format where metadata can be stored within the file) called network Common Data Form (netCDF), which means we have been able to develop numerous software tools for processing and manipulating data stored in that format. The most well known of these are a collection of command line tools known as the NetCDF Operators (NCO) and Climate Data Operators (CDO). Whenever an NCO or CDO command is executed, a time stamp followed by a copy of the command line entry is automatically placed into the global attributes of the output netCDF file, thus maintaining a history of the provenance of that data (see Box 3 for examples of NCO and CDO entries).

What is important here is not the specific software tools or file format (nobody uses NCO, CDO, or netCDF exclusively) but rather the deceptively simple method for recording previous computational steps. Using any of the programming languages common to the weather and climate sciences, a user can generate a timestamp, obtain details of the associated command line entry and append such text to the global attributes of a netCDF file (or a corresponding text file if dealing with file formats that are not self-describing). In fact, in all of these languages such tasks can be achieved with just one or two lines of additional code. IS2015 provided a log file containing a complete NCO/CDO-style history for each figure; see Box 3 for details of one of those files and the associated page on Figshare \citep{Irving2015} for the complete set.

An important feature of these log files is that they are both readable and writable by any weather and climate scientist. The computational skills required to write command line programs and link them together using a shell script, for instance, can be learned in a two-day workshop such as those run by Software Carpentry \citep{Wilson2014}. If advanced practitioners are tracking their computational steps with tools like VisTrails or PyRDM then they can certainly submit log files (or equivalent documentation) exported from those tools, but as a minimum standard it is important that elaborate tools such as these are not a requirement. By spelling out every single computational step, the log files also ensure that readers do not need to be familiar with how makefiles or other workflow management tools work in order to figure out which computational steps were executed and in what order. Other features of note include:
\begin{itemize}
\item Besides a slight amendment to the initial download entry of the log file shown in Box 3 (the default text provided by the ERA-Interim data server was not particularly self explanatory), no manual editing of its contents has been done. This means that if a reviewer asked for a slight modification to the figure, for instance, the regeneration of a new log file would be trivial. By resisting the urge to clean up the file (e.g. one might consider removing path details like \verb|/mnt/meteo0/data/simmonds/dbirving/ERAInterim/data/|) it doubles as a record that is also highly useful to the author in retracing their own steps (e.g. they could use it to recall where they stored the output data on their local machine).
\item Since it cannot be assumed that the latest version of any code repository was used to generate all the results in a paper, the unique revision number / hash value is given whenever a script written by the author was used (languages like Python, R and MATLAB are able to link with version control systems like Git, so the retrieval of the revision number can be automated).
\item When more than one input file is passed to an NCO or CDO function, the history of only one of those files is retained in the output file. On occasions where this is not appropriate (i.e. where the histories of the multiple input files are very different), it is important to ensure that the history of all input files is retained. There are a number of examples of this in the log files provided by \citet{Irving2015}. 
\end{itemize}
  
 
  
  
  
  
  
  
  
  
  
  
  
  
  