\section{How did it come to this?}

In assigning blame for the reproducibility crisis, it would be easy to focus on the weak (or absent) requirements that funding agencies and publishers place upon authors. While strengthened requirements could certainly help in motivating scientists to change their ways, most scientists are already sympathetic to ideals like open science and reproducibility. In a survey of the machine learning community, \citet{Stodden2010} found that the top reason for not sharing code was time to prepare, followed by the prospect of dealing with questions from users. That would suggest that the major factor holding scientists back is not a lack of motivation, but rather the practicalities of reproducible research seem too difficult and time consuming, particularly when the pressure to publish is so strong and unrelenting.

The bewildering array of suggested tools and best practices is one reason why an individual working in the weather and climate sciences might place reproducibility in the `too hard' basket. An appropriate solution for any given scientist no doubt exists within that collection of tools and practices, but it is heavily obscured. Consider the regular scientist described in Box 1. In consulting the literature on reproducible computational research, they would be confronted with options including data provenance tracking systems like VisTrails \citep{Freire2012} and PyRDM \citep{Jacobs2014}, software environment managers like Docker and Vagrant \citep{Stodden2014}, and even online services where your code and data can be run by others \citep{Stodden2012}. These might be fantastic options for small teams of software engineers or experienced scientific programmers who may be dealing with very large workflows (e.g. post-processing of thousands of CMIP5 model runs), complex model simulations (e.g. coupled climate models) and/or production style code (e.g. they might be developing a satellite retrieval algorithm that has high re-use potential in the wider community), but a regular scientist has neither the requisite computational experience or a research problem of sufficient scale and complexity to necessarily require and/or make use of such tools.

What is needed is an example of what reproducible computational research looks like for a regular weather/climate scientist. In other words, a minimum communication standard that all studies should meet, and that more complex studies can build upon.