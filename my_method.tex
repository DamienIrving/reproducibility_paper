\section{A possible solution}

An example of how a regular weather/climate scientist might communicate their computational results was recently published in the \textit{Journal of Climate} \citep{Irving2015}. The approach followed in that paper comprises three main components: a short computation section within the paper, a version controlled and publically available code repository and the provision of supplementary log files that capture the data processing steps taken in producing each key result. 

\subsection{Computation section}

In describing the methods used in their study, \citet{Irving2015} include a short computation section within their paper. The format for that section was based on the recommendations of \citet{Jackson2012}, who point out that there is an important difference between describing the software that was used (i.e. in enough detail to convey exactly which version) and citing it (i.e. so that the authors get appropriate academic credit). As such, the computation section consists of two distinct parts. The first is a high-level description of the software used, including citations to any papers that have been written about the software. Authors of scientific software are increasingly publishing with journals like the \textit{Journal of Open Research Software}, so it is important to give them a citation (i.e. academic credit) where it is due. This description is also useful for briefly articulating what each software item is actually used for.

The second part of the computation section provided more specific details regarding the precise version of the software used and the environment in which it was run. These details were listed in the following format: name, version number, release date, institution and DOI or URL. Finally, the computational section also provided links to the code (Section \ref{s:code_repo}) referred to in the supplementary log files (Section \ref{s:log_files}). The full computation section from \citet{Irving2015} is shown in Box 2.  

\subsection{Code repository}\label{s:code_repo}

It is important that any potential communication standard for code (a) does not substantially increase the workload of authors, and (b) is consistent with established best practices for scientific computing \citep{Wilson2014a}. With respect to the latter, it would not be appropriate to ask authors to provide a single script for each key figure or analysis, since an important practice for reducing bugs/errors is to modularize code rather than copying and pasting. This means that authors should be expected/encouraged to provide a whole library of code (i.e. a repository containing many interconnected scripts).

A related computational best practice is the use of a version control system like Git, Subversion or Mercurial. These systems can easily be linked to an online hosting service such as GitHub or Bitbucket, which is the means by which a code repository can be made publically available. Authors will often produce a pristine GitHub or Bitbucket repository for a paper they have submitted (i.e. one that contains only the code that is directly relevant to that paper), however this should not be an expectation for regular scientists who are not looking for broad-scale community uptake of their code. Not only is this a time consuming practice that would likely involve a degree of cutting and pasting, it also goes against the workflow that version control promotes. By providing a link to their everyday repository instead, authors can quickly and easily signal to readers what code was used to produce the results in their paper. The authors may go on to further develop that code in the future, so the reader then also has easy access to the latest version if they would like to use it in their own work and/or suggest improvements (referred to as a `pull request' in version control parlance). There are all sorts of extra bits and pieces of code in the everyday repository that is linked to by \citet{Irving2015}. Readers will probably never look at that code (because it is not referred to in the associated log files), but what is the harm if they do? Students and other scientists could potentially learn from it, and if I am really lucky they will notice and inform me of a bug I failed to see, saving me hours of debugging.       

In addtion to providing a link to the associated BitBucket repository, \citet{Irving2015} provide a static snapshot of the repository taken at the time the manuscript was accepted by the \textit{Journal of Climate}. This was obtained using a tool developed by a Mozilla Science Lab `Code as a Research Object' project, which allows for a tagged release/snapshot of a given GitHub repository to be archived (with a DOI) on either Figshare (http://mozillascience.github.io/code-research-object/) or Zenodo (https://guides.github.com/activities/citable-code/). The motivation for doing this was not so much to get a DOI (we are not looking for community uptake and lots of citations), but rather to have a static version available on Figshare just in case the associated GitHub repository is ever moved or deleted. FIXME: It does not provide a revision history (so git hash information not useful) so this is a nice safegard against complete data loss but could not be used instead of providing the actual repo with revision history. 

\subsection{Log files}\label{s:log_files}

A code repository on its own is not much use to a reader; they also need to know how that code was used in generating the results presented in the paper. It turns out that in the weather and climate sciences, the answer to adequately documenting the computational steps involved in producing a given result has been staring us in the face for years. As a community we have almost universally adopted a self-describing file format (i.e. a format where metadata can be stored within the file) called network Common Data Form (netCDF), which means we have been able to develop numerous software tools for processing and manipulating data stored in that format. The most well known of these are a collection of command line tools known as the netCDF operators (NCO) and climate data operators (CDO) (see Box 2 for details). Whenever an NCO or CDO command is executed, a time stamp followed by a copy of the command line entry is automatically placed into the global attributes of the output netCDF file, thus maintaining a history of the provenance of that data (see Box 3 for examples).

What is important here is not the specific software tools or file format (nobody uses NCO, CDO, or netCDF exclusively) but rather this deceptively simple method for recording previous computational steps. Using any of the programming languages common to the weather and climate sciences, whether it be Python, R, MATLAB, IDL or NCL, a user can generate a timestamp, obtain details of the associated command line entry (or produce an equivalent brief message if the process was not executed at the command line) and append such text to the global attributes of a netCDF file (or a text file if you are dealing with file formats that are not self-describing). In fact, in all these languages these tasks can be done with just one or two lines of additional code. \citet{Irving2015} provides a log file containing a complete NCO/CDO-style history for each figure and key result (see Box 3 for details of one of those files).

An important aspect of these log files is that they are both readable and writable by any weather and climate scientist. The computational skills required to write command line programs and link them together using a shell script, for instance, can be learned in a two-day workshop such as those run by Software Carpentry \citep{Wilson2014} or via equivalent online tutorials. If advanced practitioners are tracking their computational steps with tools like VisTrails or PyRDM then they can certainly submit log files exported from those tools, but as a baseline standard it is important that elaborate tools such as these are not a requirement. By spelling out every single computational step, the log files also ensure that readers do not need to be familiar with how makefiles or other workflow management tools work in order to figure out which computational steps were executed and in which order.  

FIXME: Discuss how the use of unique revision numbers / hash values is a nice addition to the NCO/CDO model, so you know the exact state of the repository when that step was executed. The Figshare snapshots don't keep the revision history, so those on their own are not sufficient. 
