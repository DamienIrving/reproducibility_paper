\section{Conclusion}

In order to combat the reproducibility crisis in published computational research, a simple procedure for communicating computational results has been demonstrated \citep{Irving2015} and its rationale discussed. The procedure involves three key additions to traditional scientific papers: a short computation section within the paper, the availability of a (preferably version controlled and publicly accessible) code repository and the provision of supplementary log files that capture the data processing steps taken in producing each key result. It should provide a starting point for weather and climate scientists (and perhaps computational scientists more generally) looking to publish reproducible research, and could be adopted as a minimum standard by relevant academic journals.

The procedure/standard was developed to be consistent with recommended computational best practices and seeks to minimize the time burden on authors, which has been identified as the most important barrier to publishing code. In particular, best practice dictates that at a minimum weather and climate scientists should be (a) writing data analysis scripts so they can re-run their analyses, (b) using version control to manage those scripts for backup and ease of sharing/collaboration and (c) storing the details of their analysis steps in the global history attribute of their netCDF data files (or following an equivalent process for other file formats) to ensure the complete provenance of their data. In order to make their published results reproducible, it follows that the minimum an author would need to do is simply make those history attributes available (i.e. via the log files) along with the associated code repository and a description of the software used to execute that code. In my experience as a Software Carpentry instructor, most weather and climate scientists are comfortable with the idea of scripting, but very few use version control or keep track of the provenance of their data. The attainment of this minimum standard would therefore involve a change to the workflow of regular weather and climate scientists, however the standard has been designed to only require skills that scientists either already have or can attain quickly and easily.  

While widespread adoption of this minimum standard would be a great starting point for reproducible research, it is worth noting that as a community we should ultimately aim much higher. By way of analogy, minimum standards in the construction industry ensure that buildings will not fall over or otherwise kill their inhabitants, but if everyone only built to those minimum standards our cities would be hugely energy inefficient. The proposed minimum standard for computational research ensures that published results are reproducible (which is a big improvement on the current state of affairs), but recreating workflows from the log files and daily code repositories of even just moderately complex analyses \citep[e.g.][]{Irving2015} would be a tedious and time consuming process. Once comfortable with the skills and processes required to meet the minimum standard, authors should seek to go beyond them to improve the comprehensibility of their published computational results, in the same way that builders should strive for a five-star energy rating. The precise tools and methods used in this endeavor will vary from author to author. Basic analyses might only require the inclusion of informative REAMDE files in the author's code repository, while others might chose to pre-package their code/software for ease of installation (e.g. for inclusion in the Python Package Index) or make their code available to run on an online platform like RunMyCode \citep{Stodden2012}. As previously mentioned, it would not be appropriate to include these many varied (and often complex) options in any minimum standards, but they represent an excellent next-step for scientists who have mastered the basics.  
