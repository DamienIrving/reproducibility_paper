As in many other research disciplines, weather and climate science has undergone a computational revolution in recent decades. Modern practitioners – most of whom are not computational experts – now primarily analyse data using a wide variety of software tools and packages. While the rapid rise of computational research has facilitated new and exciting findings, commentators note that it is impossible to replicate and verify most of today's published computational results.

This essay argues that the primary cause of the reproducibility crisis is not a lack of computational competence, adequate tooling or motivation on the part of scientists, but rather a lack of appropriate examples and protocols to follow. Most editorial/advice pieces tend to focus on production style code and/or large data processing pipelines, whereas a typical weather and climate scientist writes code for a very specific purpose (i.e. not for community uptake) and relatively modest pipelines. It is therefore not surprising that while authors are typically sympathetic to the cries of commentators, ultimately they have not changed their ways.

A simple procedure is outlined for documenting computational results, which was implemented in a recent Journal of Climate paper. It involves the inclusion of a short computation section within the paper, an externally hosted (e.g. Figshare, Zenodo) snapshot of the associated code repository and the provision of supplementary log files that capture the data processing steps taken in producing each key result. An attractive feature of the procedure is that it does not substantially increase the workload of the author, reviewers or publisher.
