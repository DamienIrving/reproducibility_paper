\section{A reproducibility crisis}
% First two sentences copied from Ince2012
The rise of computational science has led to unprecedented opportunities in the weather and climate sciences. Ever more powerful computers enable theories to be investigated that were thought almost intractable a decade ago, while new hardware technologies allow data collection in even the most inaccessible places. In order to analyse the vast quantities of data now available to them, modern practitioners – most of whom are not computational experts – use an increasingly diverse set of software tools and packages. Today's weather or climate scientist is far more likely to be found debugging code written in Python, MATLAB, IDL, NCL or R, than to be pouring over satellite images or releasing radiosondes. 
%--

This computational revolution is not unique to the weather and climate sciences, and has led to something of a reproducibility crisis in published research \citep[e.g.][]{Peng2011}. Most papers do not make the code underpinning key findings available, nor do they adequately specify the software used to execute that code. This means it is impossible to replicate and verify most of the computational results presented in journal articles today.

A movement within the computational science community has arisen in response to the crisis, calling for existing communication standards to be adpapted to include the data and code associated with published findings \citep[e.g.][]{Stodden2014}. That movement has also been active in producing best practice recommndations to guide scientists and stakeholders \citep[e.g.][]{Stodden2012,Stodden2014,Wilson2014}, and similar calls and guidelines have appeared in numerous editorials and commentaries in recent years \citep[e.g.][]{Barnes2010,Merali2010,Ince2012}. In response to this sustained campaign, there has been a mixed reaction from funding agencies, academic journals and individual scientists. Agencies like the National Science Foundation now require dataset disclosure and encourage software availability, however this is inconsistenty enforced and compliance is largely left to the authors themselves \citep{Stodden2013}. A recent review of journal policies found a trend toward data and code availability (with high impact journals like \textit{Nature} and \textit{Science} leading the way), but overall the vast majority of journals have no data or code policy \citet{Stodden2013}. In spite of the inconsistent and mostly absent requirements  , a small number of highly motivated individual scientists have taken it upon themselves to produce reproducible work (REF: gold standard papers), but in the main nothing has changed.
 
If we focus on AMS in particular, the journal has no guidelines and no researchers make their code available. Most are sympathic to the idea but don't do it. This essay argues that the primary cause of the reproducibility crisis is not a lack of computational competence (although this does play a role), adequate tooling or motivation on the part of scientists, but rather a lack of appropriate examples and protocols to follow.

%\section{Why are aren't we doing it?} Most of the advice etc out there is either very high level (e.g. http://www.siam.org/news/news.php?id=2078)

%As a result, computational science is facing a credibility crisis [1,2,4,5]. The enormous scale of state-of-the-art scientific computations, using tens or hundreds of thousands of processors, presents unprecedented challenges. Numerical reproducibility is a major issue, as is hardware reliability. For some applications, even rare interactions of circuitry with stray subatomic particles matter.

%I'm focused on code and not data, because the everyday researcher isn't collecting their own data - CMIP, reanalysis etc