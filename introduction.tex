\section{The reproducibility crisis}

The rise of computational science has led to unprecedented opportunities in the weather and climate sciences. Ever more powerful computers enable experiments that would have been considered impossible only a decade ago, while new hardware technologies allow data collection in even the most inaccessible places. In order to analyze the vast quantities of data now available to them, modern practitioners – most of whom are not computational experts – use an increasingly diverse set of software tools and packages. Today's weather or climate scientist is far more likely to be found debugging code written in Python, MATLAB, Interactive Data Language (IDL), NCAR Command Language (NCL) or R, than to be poring over satellite images or releasing radiosondes. 

This computational revolution is not unique to the weather and climate sciences and has led to something of a reproducibility crisis in published research \citep[e.g.][]{Peng2011}. Most papers do not make the data and computer code underpinning key findings available, nor do they adequately specify the software used to execute that code. This means it is impossible to replicate and verify most of the computational results presented in journal articles today.

A movement within the computational science community has arisen in response to the crisis, calling for existing communication standards to be adapted to include the data and code associated with published findings \citep[e.g.][]{Stodden2014}. That movement has also been active in producing best practice recommendations to guide scientists and stakeholders \citep[e.g.][]{Prlic2012,Sandve2013,Stodden2012a,Stodden2014}, and similar calls and guidelines have appeared in numerous editorials and commentaries in recent years \citep[e.g.][]{Barnes2010,Ince2012,Merali2010}. In response to this sustained campaign, there has been a modest but perceptible reaction from funding agencies, academic journals and individual scientists. Agencies like the National Science Foundation now require dataset disclosure and encourage software availability, however this is not consistently enforced and compliance is largely left to the authors themselves \citep{Stodden2013}. A recent review of journal policies found a trend toward data and code availability (with high impact journals like \textit{Nature} and \textit{Science} leading the way), but overall the vast majority of journals have no data or code policy \citep{Stodden2013}. Similarly, a small number of highly motivated scientists have taken it upon themselves to publish reproducible computational results \citep[e.g.][]{Crooks2014,Ketcheson2012,Schmitt2015}, but these cases are the exception as opposed to the norm.

A glance at the latest issue of any of the major journals in the weather and climate sciences reveals that researchers rarely (if ever) make their code available. Data availability is somewhat less relevant, due to the prevalence of analyses based on publicly available (and previously documented) data from the various reanalysis projects, model intercomparison projects or major institutional collections of observational data (e.g. National Snow and Ice Data Center). This essay will therefore focus exclusively on code availability, which is the component of the reproducibility crisis that is common to essentially all weather and climate research today. The societies behind most of the major journals (American Meteorological Society, Royal Meteorological Society, American Geophysical Union and European Geophysical Union) all have official data policies, however only two of the four indicate that code is included under their broad definition of data or metadata. Where code is included, statements regarding code availability consist of brief, vague suggestions that are not enforced by editors and reviewers. The reviewer guidelines for these journals will typically ask if the methodology is explained in sufficient detail for the paper's conclusions to be tested by others, but no specific guidance regarding data or code is given. 

This essay will argue that the primary cause of the reproducibility crisis is not a lack of computational competence, adequate tooling or motivation on the part of scientists, but rather a lack of simple examples to follow. One such example is provided, which represents a starting point for weather and climate scientists looking to publish reproducible computational results. It is proposed that the procedure followed in that example could be adopted as a minimum standard by relevant journals.
