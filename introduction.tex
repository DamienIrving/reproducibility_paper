\section{The reproducibility crisis}
% First two sentences copied from Ince2012
The rise of computational science has led to unprecedented opportunities in the weather and climate sciences. Ever more powerful computers enable theories to be investigated that were thought almost intractable a decade ago, while new hardware technologies allow data collection in even the most inaccessible places. In order to analyse the vast quantities of data now available to them, modern practitioners – most of whom are not computational experts – use an increasingly diverse set of software tools and packages. Today's weather or climate scientist is far more likely to be found debugging code written in Python, MATLAB, IDL, NCL or R, than to be pouring over satellite images or releasing radiosondes. 
%--

This computational revolution is not unique to the weather and climate sciences, and has led to something of a reproducibility crisis in published research \citep[e.g.][]{Peng2011}. Most papers do not make the data and computer code underpinning key findings available, nor do they adequately specify the software used to execute that code. This means it is impossible to replicate and verify most of the computational results presented in journal articles today.

A movement within the computational science community has arisen in response to the crisis, calling for existing communication standards to be adpapted to include the data and code associated with published findings \citep[e.g.][]{Stodden2014}. That movement has also been active in producing best practice recommndations to guide scientists and stakeholders \citep[e.g.][]{Sandve2013,Stodden2012a,Stodden2014}, and similar calls and guidelines have appeared in numerous editorials and commentaries in recent years \citep[e.g.][]{Barnes2010,Ince2012,Merali2010}. In response to this sustained campaign, there has been a modest but perceptible reaction from funding agencies, academic journals and individual scientists. Agencies like the National Science Foundation now require dataset disclosure and encourage software availability, however this is not consistently enforced and compliance is largely left to the authors themselves \citep{Stodden2013}. A recent review of journal policies found a trend toward data and code availability (with high impact journals like \textit{Nature} and \textit{Science} leading the way), but overall the vast majority of journals have no data or code policy \citep{Stodden2013}. Similarly, a small number of highly motivated scientists have taken it upon themselves to publish reproducible computational results \citep[e.g.][]{Crooks2014,Ketcheson2012,Schmitt2015}, but these cases are the exception as opposed to the norm.

A glance at the latest issue of any of the major journals in the weather and climate sciences reveals that researchers rarely (if ever) make their code available. Data availability is somewhat less relevant, due to the prevalence of analyses based on publically available (and previously documented) data from the various reanalysis projects, model intercomparison projects or major institutional collections of observational data (e.g. NOAA, NSIDC). This essay will therefore focus exclusively on code availability, which is the component of the reproducibility crisis that is common to essentially all weather and climate research today. The societies behind the major journals (i.e. the American Meteorological Society, Royal Meteorological Society, American Geophysical Union and European Geophysical Union) all have official data policies, however only two of the four indicate that code is included under their broad definition of code. Where code is included, statements regarding the availability consist brief, vague suggestions that are clearly not enforced by editors and reviewers. Reviewer guidelines for these journals will typically ask if the methodology is explained in sufficient detail for the paper's conclusions to be tested by others, but no specific guidance regarding data or code is given. 

This essay will argue that the primary cause of the reproducibility crisis is not a lack of computational competence (although this does play a role), adequate tooling or motivation on the part of scientists, but rather a lack of baseline standard to emulate and build upon. A simple procedure is outlined, which is proposed as a minimum standard for the documentation of computational results in the weather and climate sciences.
