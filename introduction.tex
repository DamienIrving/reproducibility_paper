\section{The reproducibility crisis}
% First two sentences copied from Ince2012
The rise of computational science has led to unprecedented opportunities in the weather and climate sciences. Ever more powerful computers enable theories to be investigated that were thought almost intractable a decade ago, while new hardware technologies allow data collection in even the most inaccessible places. In order to analyse the vast quantities of data now available to them, modern practitioners – most of whom are not computational experts – use an increasingly diverse set of software tools and packages. Today's weather or climate scientist is far more likely to be found debugging code written in Python, MATLAB, IDL, NCL or R, than to be pouring over satellite images or releasing radiosondes. 
%--

This computational revolution is not unique to the weather and climate sciences, and has led to something of a reproducibility crisis in published research \citep[e.g.][]{Peng2011}. Most papers do not make the data and computer code underpinning key findings available, nor do they adequately specify the software used to execute that code. This means it is impossible to replicate and verify most of the computational results presented in journal articles today.

A movement within the computational science community has arisen in response to the crisis, calling for existing communication standards to be adpapted to include the data and code associated with published findings \citep[e.g.][]{Stodden2014}. That movement has also been active in producing best practice recommndations to guide scientists and stakeholders \citep[e.g.][]{Sandve2013,Stodden2012a,Stodden2014}, and similar calls and guidelines have appeared in numerous editorials and commentaries in recent years \citep[e.g.][]{Barnes2010,Ince2012,Merali2010}. In response to this sustained campaign, there has been a modest but perceptible reaction from funding agencies, academic journals and individual scientists. Agencies like the National Science Foundation now require dataset disclosure and encourage software availability, however this is not consistently enforced and compliance is largely left to the authors themselves \citep{Stodden2013}. A recent review of journal policies found a trend toward data and code availability (with high impact journals like \textit{Nature} and \textit{Science} leading the way), but overall the vast majority of journals have no data or code policy \citep{Stodden2013}. Similarly, a small number of highly motivated scientists have taken it upon themselves to publish reproducible computational results \citep[e.g.][]{Crooks2014,Ketcheson2012,Schmitt2015}, but these cases are the exception as opposed to the norm.

Focusing on the weather and climate sciences in particular, the reviewer guidelines for AMS journals currently ask if "the methodology is explained in sufficient detail so that the paper's scientific conclusions could be tested by others," but no specific guidance regarding data or code is given. A glance at the latest issue of any of the AMS journals reveals that researchers rarely (if ever) make their code available. Data availability is somewhat less relevant to many weather and climate scientists, due to the prevalence of analyses based on publically available (and previously documented) data from the various reanalysis projects, model intercomparison projects or major institutional collections of observational data (e.g. NOAA, NSIDC). This essay will therefore  focus exclusively on code availability, which is the component of the reproducibility crisis that is common to essentially all weather and climate research today. It is argued that the primary cause of the reproducibility crisis is not a lack of computational competence (although this does play a role), adequate tooling or motivation on the part of scientists, but rather a lack of appropriate examples and protocols to follow. A simple procedure is outlined for documenting computational results, which does not substantially increase the workload of the author, reviewers or publisher.

%Reviewer guidelines: http://www2.ametsoc.org/ams/index.cfm/publications/editors-and-reviewers/reviewer-guidelines-for-ams-journals/ 

%\section{Why are aren't we doing it?} Most of the advice etc out there is either very high level (e.g. http://www.siam.org/news/news.php?id=2078)

%As a result, computational science is facing a credibility crisis [1,2,4,5]. The enormous scale of state-of-the-art scientific computations, using tens or hundreds of thousands of processors, presents unprecedented challenges. Numerical reproducibility is a major issue, as is hardware reliability. For some applications, even rare interactions of circuitry with stray subatomic particles matter.