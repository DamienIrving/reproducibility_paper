\section{The reproducibility crisis}

The rise of computational science has led to unprecedented opportunities in the weather and climate sciences. Ever more powerful computers enable experiments that would have been considered impossible only a decade ago, while new hardware technologies allow data collection in even the most inaccessible places. In order to analyze the vast quantities of data now available to them, modern practitioners – most of whom are not computational experts – use an increasingly diverse set of software tools and packages. Today's weather or climate scientist is far more likely to be found debugging code written in Python, MATLAB, Interactive Data Language (IDL), NCAR Command Language (NCL) or R, than to be poring over satellite images or releasing radiosondes. 

This computational revolution is not unique to the weather and climate sciences and has led to something of a reproducibility crisis in published research \citep[e.g.][]{Peng2011}. Most papers do not make the data and code underpinning key findings available, nor do they adequately specify the software packages and libraries used to execute that code. This means it is impossible to replicate and verify most of the computational results presented in journal articles today. By extension (and perhaps even more importantly), it is also impossible for readers to interrogate the data processing methodology. If a reader cannot find out which Python library was used in re-gridding a particular dataset, how can they build upon that re-gridding method and/or apply it in their own context? 

A movement within the computational science community has arisen in response to the crisis, calling for existing communication standards to be adapted to include the data and code associated with published findings \citep[e.g.][]{Stodden2014}. That movement has also been active in producing best practice recommendations to guide scientists and stakeholders \citep[e.g.][]{Prlic2012,Sandve2013,Stodden2012a,Stodden2014}, and similar calls and guidelines have appeared in numerous editorials and commentaries in recent years \citep[e.g.][]{Barnes2010,Ince2012,Merali2010}. In response to this sustained campaign, there has been a modest but perceptible reaction from funding agencies and academic journals. Agencies like the National Science Foundation now require dataset disclosure and encourage software availability, however this is not consistently enforced and compliance is largely left to the authors themselves \citep{Stodden2013}. A recent review of journal policies found a trend toward data and code availability (with high impact journals like \textit{Nature} and \textit{Science} leading the way), but overall the vast majority of journals have no data or code policy \citep{Stodden2013}. 

Similar to many other computational disciplines, in the weather and climate sciences progress on code availability is lagging behind data availability. The societies behind most of the major journals (American Meteorological Society, Royal Meteorological Society, American Geophysical Union and European Geophysical Union) all have official data policies \citep[e.g.][]{Mayernik2015}, however only two of the four indicate that code is included under their broad definition of data or metadata. Where code is included, statements regarding code availability consist of brief, vague suggestions that are not enforced by editors and reviewers. Given that much of the research conducted by weather and climate scientists is based on publicly available (and previously documented) data from the various reanalysis projects, model intercomparison projects or major institutional collections of observational data, code availability (and not data availability) is the component of the reproducibility crisis that is common to all. The lag in progress on code availability is therefore particularly concerning. 

Faced with this reproducibility crisis, it is tempting to simply decry the slow response of journals and funding agencies in updating their communication standards. The difficulty for these bodies, however, is that there is no precedent to follow. In recent years a small number of highly motivated computational scientists have taken it upon themselves to publish reproducible results \citep[e.g.][]{Crooks2014,Ketcheson2012,Schmitt2015}, however there are very few (if any) examples from the weather and climate sciences. In an attempt to address this deficiency, this essay will describe the rationale behind a reproducible paper that was recently published in the Journal of Climate \citep{Irving2016}. For the reasons articulated above \citep[and the fact that data availability has already been addressed in a recent BAMS essay][]{Mayernik2015a}, the focus will be on code availability. It will be proposed that the approach taken by \citet{Irving2016} could be adopted as minimum communication standard for code by relevant journals.  
