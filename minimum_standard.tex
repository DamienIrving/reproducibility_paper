\section{A new minimum standard}

\subsection{Implications and practicalities}

Before formally proposing a minimum standard for the communication of computational results, it is worth considering the implications and practicalities of adopting a standard based on the approach of IS2015. The reproducibility of published results would presumably improve, which may also lead to increased trust, interest and citations \citep{Piwowar2007}, but what else would it mean for authors and reviewers? Would there be barriers to overcome in implementing such a standard, and what could be done to make the transition easier?

\subsubsection{Authors}

As previously mentioned, \citet{Stodden2010} identified the time involved as the largest barrier to sharing code. There are numerous authors who suggest that the best practices adopted by IS2015 (e.g. scripting, version control) save time in the long run \citep[e.g.][]{Sandve2013,Wilson2014a}, which means that once researchers have learned and adopted these practices they may actually save time. In my experience as a Software Carpentry \citep{Wilson2014} instructor, many weather and climate scientists are comfortable with the idea of scripting, but very few use version control. Learning these new skills is not overly time consuming (Software Carpentry teaches them in a short two-day workshop), but on a local level it requires an individual or institution to take the lead in liaising with Software Carpentry to find volunteer instructors and to coordinate other logistics. A good example is the Australian Meteorological and Oceanographic Society, who have hosted a Software Carpentry workshop alongside their annual conference for the past three years running. Of course, it is also possible to learn these skills by following an online tutorial (e.g. all the Software Carpentry lessons are available online), but there is an added benefit to the social aspect of a workshop. It helps to reduce the shame many scientists have about the quality of their code (i.e. they see that their peers are no `better' at coding than they are), which is an important part of achieving the required cultural shift towards an acceptance of code sharing \citep{Barnes2010}.

The other potentially time consuming task associated with adopting a minimum standard would be dealing with requests for assistance. One suggested solution to this problem is to make it clear that authors are not obliged to support others in repeating their computations \citep{Easterbrook2014}. This is probably the only feasible solution, but it is worth noting that even if not formally obliged, some authors may fear that refusing requests will make it look like they have something to hide. 

Some researchers also face barriers relating to security and proprietary, particularly if they are using large code bases that have been developed for research and/or operations within government laboratories, national weather bureaus and private companies \citep{Stodden2010}. Such code bases are increasingly being made public (e.g. the Australian Bureau of Meteorology and CSIRO host the code for their Climate and Weather Science Laboratory in a public GitHub repository), but any proposed minimum standard would need to allow some flexibility for researchers who are unable to make their code public for these reasons (the code archived by MPI-M is available via request only, which might be an acceptable solution in many cases). For those concerned about getting appropriate academic credit for highly novel and original code, a separate publication (e.g. with the \textit{Journal of Open Research Software}) or software license \citep[e.g.][]{Stodden2009} might also be an option.

\subsubsection{Reviewers}

If the approach of IS2015 was to be implemented as a minimum standard, it would be important to convey to reviewers that they are not expected to review the code associated with a submission; they simply have to check that it is sufficiently documented (i.e. that the code is available in an online repository and that log files have been provided for all figures and key results). Not only would it be unrealistic to have reviewers examine submitted code due to the wide variety of software tools and programming languages out there, it would also be inconsistent with the way scientific methods have always been reviewed. For instance, in the 1980s it was common for weather and climate scientists to manually identify weather systems of interest (e.g. polar lows) from satellite imagery. The reviewers of the day were not required to go through all the satellite images and check that the author had counted correctly, they simply had to check that the criteria for polar low identification was adequately documented. This is not to say that counting errors were not made on the part of authors (as with computer code today there were surely numerous errors/bugs), it was just not the job of the reviewer to find them. Author errors are borne out when other studies show conflicting results and/or when other authors try to replicate key results, which is a process that would be greatly enhanced by having a minimum standard for the communication of computational results. This idea of conceptualizing the peer review of code as a post publication process is consistent with the publication system envisaged by the open evaluation movement \citep[e.g.][]{Kriegeskorte2012}. 
  
\subsection{Proposed standards}

To assist in establishing a minimum standard for the communication of computational results, it is proposed that the following could be inserted into the author and reviewer guidelines of journals in the weather and climate sciences (institutions that have their own internal review process could also adopt the standards). In places the language borrows from the guidelines recently adopted by \textit{Nature} \citep{Nature2014}. It is anticipated that a journal would provide links to examples of well documented computational results to help both authors and reviewers in complying with these guidelines. The journal could decide to host the supplementary software description, log files and static snapshot of the relevant code itself, or encourage the author to host these items at an external location that can guarantee persistent, long-term access (e.g. an institutionally supported site like MPI-M provides for its researchers or an online academic archive such as Figshare or Zenodo).

\subsubsection{Author guidelines}

If computer code is central to any of the paper's major conclusions, then the following is required as a minimum standard: 
\begin{enumerate}
\item A statement describing whether (and where) that code is available and setting out any restrictions on accessibility. 
\item A high-level description of the software used to execute that code (including citations for any academic papers written to describe that software).
\item A supplementary file outlining the precise version of the software packages and operating system used. This information should be presented in the following format: name, version number, release date, institution, DOI or URL.
\item A supplementary log file for each major result (including key figures) listing all computational steps taken from the initial download/attainment of the data to the final result (i.e. the log files describe how the code and software were used to produce the major results). 
\end{enumerate}

It is recommended that items 1 and 2 are included in a `Computation Procedures' (or similarly named) section within the manuscript itself. Any practical issues preventing code sharing will be evaluated by the editors, who reserve the right to decline a paper if important code is unavailable. While not a compulsory requirement, best practice for code sharing involves managing code with a version control system such as Git, Subversion or Mercurial, which is then linked to a publicly accessible online repository such as GitHub or Bitbucket. In the log files a unique revision number (or hash value) can then be quoted to indicate the precise version of the code repository that was used. Authors are not expected to produce a brand new repository to accompany their paper; an `everyday' repository which also contains code not relevant to the paper is acceptable. Authors should also note that they are not obliged to support reviewers or readers in repeating their computations.

\subsubsection{Reviewer guidelines}

The reviewer guidelines for most journals already ask if the methodology is explained in sufficient detail so that the paper's scientific conclusions could be tested by others. Such guidelines could simply be added to as follows: `If computer code is central to any of those conclusions, then reviewers should ensure that the authors are compliant with the minimum standards outlined in the author guidelines. It should be noted that reviewers are not obliged to assess or execute the code associated with a submission. They must simply check that it is adequately documented.'   
  
 
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  