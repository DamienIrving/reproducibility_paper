\section{A new minimum standard}

\subsection{Implications and practicalities}

Before formally proposing a minimum standard for the communication of computational results, it is worth considering the implications and practicalities of adopting a standard based on the approach of IS2015. The reproducibility of published results would presumably improve, which may also lead to increased trust, interest and citations \citep{Piwowar2007}, but what else would it mean for authors and reviewers? Would there be barriers to overcome in implementing such a standard, and what could be done to make the transition easier?

\subsubsection{Authors}

There are numerous authors who suggest that best practices like scripting and version control save researchers time in the long run \citep[e.g.][]{Sandve2013,Wilson2014a}, so the procedure outlined above is not a time consuming burden in that respect, particularly as they are skills that researchers either already have or can pick up very quickly. What might be time consuming is dealing with requests for assistance, which was a concern identified by \citet{Stodden2010}. One suggested solution to this problem is to make it clear that authors are not obliged to support others in repeating their computations \citep{Easterbrook2014}. This is probably the only feasible solution, but it is worth noting that even if not formally obliged, some authors may fear that refusing requests will make it look like they have something to hide. Another potential barrier is the shame that many scientists have about the quality of their code. Widespread sharing of code would no doubt improve this situation (i.e. scientists would see that their peers are no better at coding than they are), but the transition to regular code sharing would require a slight cultural shift within the profession \citep{Barnes2010}.

\subsubsection{Reviewers}

If the approach of IS2015 was to be implemented as a minimum standard, it would be important to convey to reviewers that they are not expected to review the code associated with a submission; they simply have to check that it is sufficiently documented (i.e. that the code is available in an online repository and that log files have been provided for all figures and key results). Not only would it be unrealistic to have reviewers examine submitted code due to the wide variety of software tools and programming languages out there, it would also be inconsistent with the way scientific methods have always been reviewed. For instance, in the 1980s it was common for weather and climate scientists to manually identify weather systems of interest (e.g. polar lows) from satellite imagery. The reviewers of the day were not required to go through all the satellite images and check that the author had counted correctly, they simply had to check that the criteria for polar low identification was adequately documented. This is not to say that counting errors were not made on the part of authors (as with computer code today there were surely numerous errors/bugs), it was just not the job of the reviewer to root them out. Author errors are borne out when other studies show conflicting results and/or when other authors try to replicate key results, which is a process that would be greatly enhanced by having a minimum standard for the communication of computational results. This idea of conceptualizing the peer review of code as a post publication process is consistent with the publication system envisaged by the open evaluation movement \citep[e.g.][]{Kriegeskorte2012}. 
  
\subsection{Proposed standards}

To assist in the establishment a minimum standard for the communication of computational results, it is proposed that the following could be inserted into the author and reviewer guidelines of journals in the weather and climate sciences. In places the language borrows from the guidelines recently adopted by \textit{Nature} \citep{Nature2014}. It is anticipated that a journal would also provide links to examples of well documented computational results to help both authors and reviewers in complying with these guidelines.

\subsubsection{Author guidelines}

If computer code is central to any of the paper's major conclusions, then the following is required as a minimum standard: 
\begin{enumerate}
\item A statement describing whether (and where) that code is available and setting out any restrictions on accessibility. Best practice involves managing code with a version control system such as Git, Subversion or Mercurial, which is then linked to a publicly accessible online repository such as GitHub or Bitbucket. Authors are not expected to produce a brand new repository to accompany their paper; an `everyday' repository which also contains code not relevant to the paper is fine.  
\item A high-level description of the software used to execute that code (including citations for any academic papers written to describe that software).
\item A supplementary file outlining the precise version of the software packages and operating system used. This information should be presented in the following format: name, version number, release date, institution, DOI or URL.
\item A supplementary log file for each major result (including key figures) showing all computational steps taken from the initial download/attainment of the data to the final result. Best practice involves recording the time/date each step was executed as well as the unique revision number (or hash value) indicating which version of the code repository was used. 
\end{enumerate}

It is recommended that items 1 and 2 are included in a `Computation Procedures' (or similarly named) section with manuscript itself, while the supplementary files can be hosted at an online academic archive such as Figshare or Zenodo. Any practical issues preventing code sharing will be evaluated by the editors, who reserve the right to decline a paper if important code is unavailable. Authors should note that they are not obliged to support reviewers or readers in repeating their computations.

\subsubsection{Reviewer guidelines}

The reviewer guidelines for most journals already ask if the methodology is explained in sufficient detail so that the paper's scientific conclusions could be tested by others. Such guidelines could simply be added to as follows: `If computer code is central to any of those conclusions, then reviewers should ensure that the authors are compliant with the minimum standards outlined in the author guidelines. It should be noted that reviewers are not obliged to assess or execute the code associated with a submission. They must simply check that it is adequately documented.'   
  
 
    
  
  
  
  