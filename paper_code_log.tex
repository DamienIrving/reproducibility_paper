\subsubsection{Code repository}

It is important that any minimum communication standard for code (a) does not substantially increase the workload of authors, and (b) is consistent with established best practices for scientific computing \citep{Wilson2014a}. With respect to the latter, it would not be appropriate to ask authors to provide a single script for each key figure or analysis, since an important practice for reducing bugs/errors is to modularize code rather than copying and pasting. This means that authors should be expected/encouraged to provide a whole library of code (i.e. a repository containing many interconnected scripts).

A related computational best practice is the use of a version control system like Git, Subversion or Mercurial. These systems are easily linked to an online hosting service such as GitHub or Bitbucket, which is the means by which a code repository can be made publicly available. Most of the examples of reproducible research currently available in the literature produced a pristine GitHub or Bitbucket repository for their paper (i.e. one that contains only the code that is directly relevant to that paper), however this should not be an expectation for regular scientists who are not looking for broad-scale community uptake of their code. Not only is this a time consuming practice that would likely involve a degree of cutting and pasting, it also goes against the workflow that version control promotes. By providing a link to their everyday repository instead, authors can quickly and easily signal to readers what code was used to produce the results in their paper. The authors may further develop that code in the future, so the reader then also has easy access to the latest version if they would like to use it in their own work and/or suggest improvements (referred to as a `pull request' in version control parlance). There are all sorts of extra bits and pieces of code in the everyday repository that is linked to by \citet{Irving2016}. Readers will probably never look at that code (because it is not referred to in the associated log files), but what is the harm if they do? Students and other scientists could potentially learn from it, and if I am really lucky they will notice and inform me of a bug I failed to see, saving me hours of debugging.       

In addition to providing a link to the associated Github repository, \citet{Irving2016} provide (on Figshare) a static snapshot of the repository taken at the time the manuscript was accepted for publication. The motivation for doing this was not so much to obtain the Digital Object Identifier (DOI) that comes with Figshare pages (because we are not looking for community uptake and lots of citations), but rather to have a static version of the code available in case the associated GitHub repository is ever moved or deleted. Since this snapshot does not provide a revision history of the code, it would not have been appropriate to provide it \textit{instead} of the link to GitHub. As the log file in Box 3 demonstrates, not all of the results shown in a paper will necessarily have been generated with the very latest version of the code, hence the need for the revision history. In other words, the version controlled repository is of critical importance, while the static snapshot is simply an additional backup measure.  

\subsubsection{Log files}\label{s:log_files}

A code repository on its own is not much use to a reader; they also need to know how that code was used in generating the results presented in the paper. It turns out that in the weather and climate sciences, the answer to adequately documenting the computational steps involved in producing a given result has been staring us in the face for years. As a community we have almost universally adopted a self-describing file format (i.e. a format where metadata can be stored within the file) called network Common Data Form (netCDF), which means we have been able to develop numerous software tools for processing and manipulating data stored in that format. The most well known of these are a collection of command line tools known as the NetCDF Operators (NCO) and Climate Data Operators (CDO) (see Box 2 for details). Whenever an NCO or CDO command is executed, a time stamp followed by a copy of the command line entry is automatically placed into the global attributes of the output netCDF file, thus maintaining a history of the provenance of that data (see Box 3 for examples of NCO and CDO entries).

What is important here is not the specific software tools or file format (nobody uses NCO, CDO, or netCDF exclusively) but rather the deceptively simple method for recording previous computational steps. Using any of the programming languages common to the weather and climate sciences, a user can generate a timestamp, obtain details of the associated command line entry (or produce an equivalent brief message if the process was not executed at the command line) and append such text to the global attributes of a netCDF file (or a corresponding text file if dealing with file formats that are not self-describing). In fact, in all of these languages such tasks can be achieved with just one or two lines of additional code. \citet{Irving2016} provide a log file containing a complete NCO/CDO-style history for each figure and key result; see Box 3 for details of one of those files and the associated page on Figshare \citep{Irving2015} for the complete set.

An important aspect of these log files is that they are both readable and writable by any weather and climate scientist. The computational skills required to write command line programs and link them together using a shell script, for instance, can be learned in a two-day workshop such as those run by Software Carpentry \citep{Wilson2014}. If advanced practitioners are tracking their computational steps with tools like VisTrails or PyRDM then they can certainly submit log files (or equivalent documentation) exported from those tools, but as a minimum standard it is important that elaborate tools such as these are not a requirement. By spelling out every single computational step, the log files also ensure that readers do not need to be familiar with how makefiles or other workflow management tools work in order to figure out which computational steps were executed and in what order. 

The most important feature of the log file shown in Box 3 is that besides a slight amendment to the initial download entry (the default text provided by the ERA-Interim data server was not particularly self explanatory), no manual editing of its contents has been done. This means that if a reviewer asked for a slight modification to the figure, for instance, the regeneration of a new log file would be trivial. By resisting the urge to clean up the file (e.g. one might consider removing path details like \verb|/mnt/meteo0/data/simmonds/dbirving/ERAInterim/data/|) it doubles as a record that is also highly useful to the author in retracing their own steps (e.g. they could use it to recall where they stored the output data on their local machine). Other features of note include:
\begin{itemize}
\item Since it cannot be assumed that the latest version of any code repository was used to generate all the results in a paper, the unique revision number / hash value is given whenever a script written by the author was used (languages like Python, R and MATLAB are able to link with version control systems like Git, so the retrieval of the revision number can be automated).
\item When more than one input file is passed to an NCO or CDO function, the history of only one of those files is retained in the output file. On occasions where this is not appropriate (i.e. where the histories of the multiple input files are very different), it is important to ensure that the history of all input files is retained. There are a number of examples of this in the log files provided by \citet{Irving2015}. 
\end{itemize}
  
  