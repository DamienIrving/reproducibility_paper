\section{Implications}

Before formally proposing a minimum standard for the communication of computational results (Section \ref{s:guidelines}), it is worth considering what the implications might be if all researchers followed a similar approach to \citet{Irving2015}. The reproducibility of published results would obviously improve, which may also lead to increased trust, interest and citations \citep{Piwowar2007}, but what else would it mean for authors, reviewers and publishers? Are there certain things that could be done to make the transition easier for these groups?

\subsection{Authors}

There are numerous authors who suggest that best practices like the use of version control for managing code and build tools for automating workflows (e.g. shell scripts, makefiles) save researchers time in the long run \citep[e.g.][]{Sandve2013,Wilson2014a}, so the procedure outlined above is not a burden on authors in that respect (particularly as they are skills that researchers already have or that can be picked up very quickly). Perhaps the most important consideration comes from \citet{Easterbrook2014} \citep[and is supported by the findings of][]{Stodden2010}, who suggests that making code available can only work on the understanding that the author is not obliged to support others in repeating their computations. They can if they would like, but this should not be an obligation.

\subsection{Reviewers}

If the \citet{Irving2015} approach was to be implemented as a minimum standard, it would be important to convey to reviewers that they are not expected to review the code associated with a submission. They simply have to check that it is sufficiently documented (i.e. that the code is available in an online repository and that log files have been provided for all figures and key results). Not only would it be unrealistic to have reviewers examine submitted code due to the wide variety of software tools and programming languages out there, it would also be inconsistent with the way scientific methods have always been reviewed. For instance, in the 1980s it was common for weather and climate scientists to manually identify weather systems of interest (e.g. polar lows) from satellite imagery. The reviewers of the day were not required to go through all the satellite images and check that the author had counted correctly, they simply had to check that the criteria for polar low identification was adequately documented. This is not to say that counting errors were not made on the part of authors (as with computer code today there were surely numerous errors/bugs), it was just not the job of the reviewer to root them out. Author errors are borne out when other studies show conflicting results and/or when other authors try to replicate key results, which is a process that would be greatly enhanced by having a minimum standard for the communication of computational results.

\subsection{Publishers}

Publishers typically already provide online space for authors to include supplementary information, so the proposed log files would be simply make use of this space. 