\section{Implications}

Before formally suggesting a minimum standard for the communication of computational results (Section \ref{s:guidelines}), it is worth considering what the implications would be for authors, reviewers and publishers if everyone FIXME did what \citet{Irving2015} did. Clearly a minimum standard would improve the reproducibility of published results, and there is also evidence to suggest that this improved reproducibility can lead to increased trust, interest and citations \citep{Piwowar2007}. But at what cost?

\subsection{Authors}

FIXME: Add note that \citet{Stodden2010} find that the top reasons computational scientists do not share code is: time to prepare, dealing with questions from users, concerns over use without citation, possibility of patents or other IP constraints and legal barries such as copyright. I can address most of those.

There are numerous authors who suggest that best practices such as the use of version control for managing code and build tools (e.g. shell scripts, makefiles) for automating workflows save researchers time in the long run \citep[e.g.][]{Sandve2013,Wilson2014a}, so the proposed minimum standard is not a burden on authors in that respect, particularly as they are skills that researchers already have or that can be picked up very quickly. Perhaps the most important consideration comes from \citet{Easterbrook2014}, who suggests that making code available can only work on the understanding that the author is not obliged to support others in repeating their computations. They can if they would like, but this should not be an obligation.

\subsection{Reviewers}

In implementing the minimum standard, it would be important to convey to reviewers that they are not expected to review the code associated with a submission. They simply have to check that it is sufficiently documented (i.e. that the code is available in an online repository and that log files have been provided for all figures and key results). Not only would it be unrealistic to have reviewers examine submitted code due to the wide variety of software tools and programming languages out there, it would also be inconsistent with the way scientific methods have always been reviewed. For instance, in the 1980s it was common for weather and climate scientists to manually identify weather systems of interest (e.g. polar lows) from satellite imagery. The reviewers of the day were not required to go through all the satellite images and check that the author had counted correctly, they simply had to check that the criteria for polar low identification was adequately documented. This is not to say that counting errors were not made on the part of authors (as with computer code today there were surely numerous errors/bugs), it is just not the job of the reviewer to root them out. Author errors are borne out when other studies show conflicting results and/or when other authors try to replicate key results, which is a process that is greatly enhanced by having a minimum standard for the communication of computational results.

\subsection{Publishers}

Publishers typically already provide space online for authors to include supplementary information, so the proposed log files would be simply make use of this space. 