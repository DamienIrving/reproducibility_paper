\section{Implications}

Before formally proposing a minimum standard for the communication of computational results (Section \ref{s:guidelines}), it is worth considering the implications and practicalities of adopting a standard based on the approach of \citet{Irving2015}. The reproducibility of published results would presumably improve, which may also lead to increased trust, interest and citations \citep{Piwowar2007}, but what else would it mean for authors, reviewers and publishers? Would there be barriers to overcome in implementing such a standard, and what could be done to make the transition easier?

\subsection{Authors}

There are numerous authors who suggest that best practices like version control and build tools for automating workflows (e.g. shell scripts, makefiles) save researchers time in the long run \citep[e.g.][]{Sandve2013,Wilson2014a}, so the procedure outlined above is not a time consuming burden in that respect, particularly as they are skills that researchers either already have or can pick up very quickly. What might be time consuming is dealing with requests for assistance in running the code associated with a paper, which was a major concern documented by \citet{Stodden2010}. One suggested solution to this problem is to make it clear that authors are not obliged to support others in repeating their computations \citep{Easterbrook2014}. This is probably the only feasible solution, but it is worth noting that even if not formally obliged, some authors may fear that refusing requests will make it look like they have something to hide. Another potential barrier is the shame that many scientists have about the quality of their code. Widespread sharing of code would no doubt improve this situation (i.e. scientists could see that others are just as bad as they are), but the transition to regular code sharing would require a slight cultural shift within the profession \citep{Barnes2010}.

\subsection{Reviewers}

If the \citet{Irving2015} approach was to be implemented as a minimum standard, it would be important to convey to reviewers that they are not expected to review the code associated with a submission; they simply have to check that it is sufficiently documented (i.e. that the code is available in an online repository and that log files have been provided for all figures and key results). Not only would it be unrealistic to have reviewers examine submitted code due to the wide variety of software tools and programming languages out there, it would also be inconsistent with the way scientific methods have always been reviewed. For instance, in the 1980s it was common for weather and climate scientists to manually identify weather systems of interest (e.g. polar lows) from satellite imagery. The reviewers of the day were not required to go through all the satellite images and check that the author had counted correctly, they simply had to check that the criteria for polar low identification was adequately documented. This is not to say that counting errors were not made on the part of authors (as with computer code today there were surely numerous errors/bugs), it was just not the job of the reviewer to root them out. Author errors are borne out when other studies show conflicting results and/or when other authors try to replicate key results, which is a process that would be greatly enhanced by having a minimum standard for the communication of computational results.

\subsection{Publishers}

Publishers typically already provide online space for authors to include supplementary information, so the proposed log files would be simply make use of this space. 